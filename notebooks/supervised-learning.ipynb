{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "import enum\n",
    "import json\n",
    "\n",
    "class EducationLevels(str, enum.Enum):\n",
    "    HIGH_SCHOOL = \"high_school\"\n",
    "    BACHELORS = \"bachelors\"\n",
    "    MASTERS = \"masters\"\n",
    "    PHD = \"phd\"\n",
    "    NONE = \"none\"\n",
    "\n",
    "class Location(BaseModel):\n",
    "    city: str\n",
    "    state_or_province: str\n",
    "    country: str\n",
    "\n",
    "class FakeProfile(BaseModel):\n",
    "    name: str\n",
    "    occupation: str\n",
    "    industry: str\n",
    "    job_description: str\n",
    "    education: EducationLevels\n",
    "    major: Optional[str] = Field(default=None)\n",
    "    location: Location\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, data: str):\n",
    "        return cls(**json.loads(data))\n",
    "\n",
    "class FakeProfiles(BaseModel):\n",
    "    profiles: List[FakeProfile]\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, data: str):\n",
    "        return cls(**json.loads(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWSGROUPS = [\n",
    "    'alt.atheism',\n",
    "    'comp.windows.x',\n",
    "    'misc.forsale',\n",
    "    'rec.autos',\n",
    "    'sci.med',\n",
    "    'rec.sport.hockey',\n",
    "    'sci.space',\n",
    "    'soc.religion.christian',\n",
    "    'talk.politics.guns'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'sci.med', 'rec.sport.hockey', 'sci.space', 'soc.religion.christian', 'talk.politics.guns']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, random_state=2, return_X_y=True)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, random_state=2, return_X_y=True)\n",
    "\n",
    "num_articles_newsgroups_train = len(newsgroups_train[0])\n",
    "num_articles_newsgroups_test = len(newsgroups_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load profiles data, pair them with articles, and store in a dataframe with corresponding output (True/False depending on whether the article is relevant to the profile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>occupation</th>\n",
       "      <th>industry</th>\n",
       "      <th>job_description</th>\n",
       "      <th>education</th>\n",
       "      <th>major</th>\n",
       "      <th>location</th>\n",
       "      <th>news_group</th>\n",
       "      <th>article</th>\n",
       "      <th>is_relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thomas Reed</td>\n",
       "      <td>Journalist</td>\n",
       "      <td>Media</td>\n",
       "      <td>Writes articles on various social topics, incl...</td>\n",
       "      <td>EducationLevels.BACHELORS</td>\n",
       "      <td>Journalism</td>\n",
       "      <td>{'city': 'Austin', 'state_or_province': 'Texas...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>From: cdash@moet.cs.colorado.edu (Charles Shub...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samantha Brooks</td>\n",
       "      <td>College Professor</td>\n",
       "      <td>Education</td>\n",
       "      <td>Teaches courses on philosophy, including metap...</td>\n",
       "      <td>EducationLevels.PHD</td>\n",
       "      <td>Philosophy</td>\n",
       "      <td>{'city': 'Berkeley', 'state_or_province': 'Cal...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Subject: Diffs to sci.space/sci.astro Frequent...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marcus Li</td>\n",
       "      <td>Software Developer</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Develops mobile applications with a focus on s...</td>\n",
       "      <td>EducationLevels.BACHELORS</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>{'city': 'Seattle', 'state_or_province': 'Wash...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>From: reid@cs.uiuc.edu (Jon Reid)\\nSubject: Ce...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emily Nguyen</td>\n",
       "      <td>Human Rights Activist</td>\n",
       "      <td>NGO</td>\n",
       "      <td>Advocates for freedom of belief and expression...</td>\n",
       "      <td>EducationLevels.MASTERS</td>\n",
       "      <td>International Relations</td>\n",
       "      <td>{'city': 'New York', 'state_or_province': 'New...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>From: jacquier@gsbux1.uchicago.edu (Eric Jacqu...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jordan Smith</td>\n",
       "      <td>Blogger</td>\n",
       "      <td>Digital Media</td>\n",
       "      <td>Runs a popular blog discussing religion, athei...</td>\n",
       "      <td>EducationLevels.BACHELORS</td>\n",
       "      <td>English</td>\n",
       "      <td>{'city': 'Denver', 'state_or_province': 'Color...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>From: sp1henhj@edit (Henrik Balthazar Hjort)\\n...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name             occupation       industry  \\\n",
       "0      Thomas Reed             Journalist          Media   \n",
       "1  Samantha Brooks      College Professor      Education   \n",
       "2        Marcus Li     Software Developer     Technology   \n",
       "3     Emily Nguyen  Human Rights Activist            NGO   \n",
       "4     Jordan Smith                Blogger  Digital Media   \n",
       "\n",
       "                                     job_description  \\\n",
       "0  Writes articles on various social topics, incl...   \n",
       "1  Teaches courses on philosophy, including metap...   \n",
       "2  Develops mobile applications with a focus on s...   \n",
       "3  Advocates for freedom of belief and expression...   \n",
       "4  Runs a popular blog discussing religion, athei...   \n",
       "\n",
       "                   education                    major  \\\n",
       "0  EducationLevels.BACHELORS               Journalism   \n",
       "1        EducationLevels.PHD               Philosophy   \n",
       "2  EducationLevels.BACHELORS         Computer Science   \n",
       "3    EducationLevels.MASTERS  International Relations   \n",
       "4  EducationLevels.BACHELORS                  English   \n",
       "\n",
       "                                            location   news_group  \\\n",
       "0  {'city': 'Austin', 'state_or_province': 'Texas...  alt.atheism   \n",
       "1  {'city': 'Berkeley', 'state_or_province': 'Cal...  alt.atheism   \n",
       "2  {'city': 'Seattle', 'state_or_province': 'Wash...  alt.atheism   \n",
       "3  {'city': 'New York', 'state_or_province': 'New...  alt.atheism   \n",
       "4  {'city': 'Denver', 'state_or_province': 'Color...  alt.atheism   \n",
       "\n",
       "                                             article  is_relevant  \n",
       "0  From: cdash@moet.cs.colorado.edu (Charles Shub...        False  \n",
       "1  Subject: Diffs to sci.space/sci.astro Frequent...        False  \n",
       "2  From: reid@cs.uiuc.edu (Jon Reid)\\nSubject: Ce...        False  \n",
       "3  From: jacquier@gsbux1.uchicago.edu (Eric Jacqu...        False  \n",
       "4  From: sp1henhj@edit (Henrik Balthazar Hjort)\\n...        False  "
      ]
     },
     "execution_count": 769,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "profiles = []\n",
    "\n",
    "# Load the profiles from the json files\n",
    "for news_group in NEWSGROUPS:\n",
    "    \n",
    "    with open(f\"../fake_profiles/{news_group.replace('.', '_')}.json\", \"r\", encoding='utf-8') as f:\n",
    "        \n",
    "        # get profiles for current news group\n",
    "        profiles_group = FakeProfiles.from_json(f.read())\n",
    "        \n",
    "        # store all profiles in dataframe\n",
    "        for profile in profiles_group.profiles:\n",
    "            # convert profile to dict\n",
    "            profile_dict = profile.model_dump()\n",
    "\n",
    "            # attach true news group to profile\n",
    "            profile_dict['news_group'] = news_group\n",
    "            \n",
    "            # get random article from train set\n",
    "            index = np.random.choice(num_articles_newsgroups_train)\n",
    "            random_article = newsgroups_train[0][index]\n",
    "            article_label = newsgroups_train[1][index]\n",
    "            \n",
    "            # determine whether article relevant (same category)\n",
    "            is_relevant = (article_label == categories.index(news_group))\n",
    "            \n",
    "            # attach article and is_relevant to profile\n",
    "            profile_dict['article'] = random_article\n",
    "            profile_dict['is_relevant'] = is_relevant\n",
    "            \n",
    "            # append profile to list           \n",
    "            profiles.append(profile_dict)\n",
    "            \n",
    "        \n",
    "df = pd.DataFrame(profiles)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for model fitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing text fields for each profile. Techniques include stopword removal and lemmatization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sevag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sevag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_shorter_text(text):\n",
    "    # remove blanks and convert to lower case\n",
    "    if text is None:\n",
    "        return 'none'\n",
    "    else:\n",
    "        return text.lower().strip()\n",
    "\n",
    "# more preprocessing for longer text\n",
    "def preprocess_longer_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    lemmatized_text = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess fields and add back to the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>occupation</th>\n",
       "      <th>industry</th>\n",
       "      <th>job_description</th>\n",
       "      <th>education</th>\n",
       "      <th>major</th>\n",
       "      <th>location</th>\n",
       "      <th>news_group</th>\n",
       "      <th>article</th>\n",
       "      <th>is_relevant</th>\n",
       "      <th>job_description_preprocessed</th>\n",
       "      <th>city</th>\n",
       "      <th>state_or_province</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thomas Reed</td>\n",
       "      <td>journalist</td>\n",
       "      <td>media</td>\n",
       "      <td>Writes articles on various social topics, incl...</td>\n",
       "      <td>EducationLevels.BACHELORS</td>\n",
       "      <td>journalism</td>\n",
       "      <td>{'city': 'Austin', 'state_or_province': 'Texas...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>From : cdash @ moet.cs.colorado.edu ( Charles ...</td>\n",
       "      <td>False</td>\n",
       "      <td>Writes article various social topic , includin...</td>\n",
       "      <td>austin</td>\n",
       "      <td>texas</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samantha Brooks</td>\n",
       "      <td>college professor</td>\n",
       "      <td>education</td>\n",
       "      <td>Teaches courses on philosophy, including metap...</td>\n",
       "      <td>EducationLevels.PHD</td>\n",
       "      <td>philosophy</td>\n",
       "      <td>{'city': 'Berkeley', 'state_or_province': 'Cal...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Subject : Diffs sci.space/sci.astro Frequently...</td>\n",
       "      <td>False</td>\n",
       "      <td>Teaches course philosophy , including metaphys...</td>\n",
       "      <td>berkeley</td>\n",
       "      <td>california</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marcus Li</td>\n",
       "      <td>software developer</td>\n",
       "      <td>technology</td>\n",
       "      <td>Develops mobile applications with a focus on s...</td>\n",
       "      <td>EducationLevels.BACHELORS</td>\n",
       "      <td>computer science</td>\n",
       "      <td>{'city': 'Seattle', 'state_or_province': 'Wash...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>From : reid @ cs.uiuc.edu ( Jon Reid ) Subject...</td>\n",
       "      <td>False</td>\n",
       "      <td>Develops mobile application focus social netwo...</td>\n",
       "      <td>seattle</td>\n",
       "      <td>washington</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emily Nguyen</td>\n",
       "      <td>human rights activist</td>\n",
       "      <td>ngo</td>\n",
       "      <td>Advocates for freedom of belief and expression...</td>\n",
       "      <td>EducationLevels.MASTERS</td>\n",
       "      <td>international relations</td>\n",
       "      <td>{'city': 'New York', 'state_or_province': 'New...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>From : jacquier @ gsbux1.uchicago.edu ( Eric J...</td>\n",
       "      <td>False</td>\n",
       "      <td>Advocates freedom belief expression around wor...</td>\n",
       "      <td>new york</td>\n",
       "      <td>new york</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jordan Smith</td>\n",
       "      <td>blogger</td>\n",
       "      <td>digital media</td>\n",
       "      <td>Runs a popular blog discussing religion, athei...</td>\n",
       "      <td>EducationLevels.BACHELORS</td>\n",
       "      <td>english</td>\n",
       "      <td>{'city': 'Denver', 'state_or_province': 'Color...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>From : sp1henhj @ edit ( Henrik Balthazar Hjor...</td>\n",
       "      <td>False</td>\n",
       "      <td>Runs popular blog discussing religion , atheis...</td>\n",
       "      <td>denver</td>\n",
       "      <td>colorado</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name             occupation       industry  \\\n",
       "0      Thomas Reed             journalist          media   \n",
       "1  Samantha Brooks      college professor      education   \n",
       "2        Marcus Li     software developer     technology   \n",
       "3     Emily Nguyen  human rights activist            ngo   \n",
       "4     Jordan Smith                blogger  digital media   \n",
       "\n",
       "                                     job_description  \\\n",
       "0  Writes articles on various social topics, incl...   \n",
       "1  Teaches courses on philosophy, including metap...   \n",
       "2  Develops mobile applications with a focus on s...   \n",
       "3  Advocates for freedom of belief and expression...   \n",
       "4  Runs a popular blog discussing religion, athei...   \n",
       "\n",
       "                   education                    major  \\\n",
       "0  EducationLevels.BACHELORS               journalism   \n",
       "1        EducationLevels.PHD               philosophy   \n",
       "2  EducationLevels.BACHELORS         computer science   \n",
       "3    EducationLevels.MASTERS  international relations   \n",
       "4  EducationLevels.BACHELORS                  english   \n",
       "\n",
       "                                            location   news_group  \\\n",
       "0  {'city': 'Austin', 'state_or_province': 'Texas...  alt.atheism   \n",
       "1  {'city': 'Berkeley', 'state_or_province': 'Cal...  alt.atheism   \n",
       "2  {'city': 'Seattle', 'state_or_province': 'Wash...  alt.atheism   \n",
       "3  {'city': 'New York', 'state_or_province': 'New...  alt.atheism   \n",
       "4  {'city': 'Denver', 'state_or_province': 'Color...  alt.atheism   \n",
       "\n",
       "                                             article  is_relevant  \\\n",
       "0  From : cdash @ moet.cs.colorado.edu ( Charles ...        False   \n",
       "1  Subject : Diffs sci.space/sci.astro Frequently...        False   \n",
       "2  From : reid @ cs.uiuc.edu ( Jon Reid ) Subject...        False   \n",
       "3  From : jacquier @ gsbux1.uchicago.edu ( Eric J...        False   \n",
       "4  From : sp1henhj @ edit ( Henrik Balthazar Hjor...        False   \n",
       "\n",
       "                        job_description_preprocessed      city  \\\n",
       "0  Writes article various social topic , includin...    austin   \n",
       "1  Teaches course philosophy , including metaphys...  berkeley   \n",
       "2  Develops mobile application focus social netwo...   seattle   \n",
       "3  Advocates freedom belief expression around wor...  new york   \n",
       "4  Runs popular blog discussing religion , atheis...    denver   \n",
       "\n",
       "  state_or_province country  \n",
       "0             texas     usa  \n",
       "1        california     usa  \n",
       "2        washington     usa  \n",
       "3          new york     usa  \n",
       "4          colorado     usa  "
      ]
     },
     "execution_count": 771,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple preprocessing for occupation, industry, major\n",
    "shorter_text_fields = ['occupation', 'industry', 'major']\n",
    "for field in shorter_text_fields:\n",
    "    df[field] = df[field].apply(preprocess_shorter_text)\n",
    "    \n",
    "# preprocessing for longer text fields\n",
    "df['job_description_preprocessed'] = df['job_description'].apply(preprocess_longer_text)\n",
    "df['article'] = df['article'].apply(preprocess_longer_text)\n",
    "    \n",
    "# flatten location\n",
    "df['city'] = df['location'].apply(lambda x: preprocess_shorter_text(x['city']))\n",
    "df['state_or_province'] = df['location'].apply(lambda x: preprocess_shorter_text(x['state_or_province']))\n",
    "df['country'] = df['location'].apply(lambda x: preprocess_shorter_text(x['country']))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting models to the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract numerical and categorical features from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# vectorization for job description\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_job_description = vectorizer.fit_transform(df['job_description_preprocessed'])\n",
    "X_article = vectorizer.fit_transform(df['article'])\n",
    "\n",
    "# one-hot encoding for education\n",
    "X_education = pd.get_dummies(df['education'])\n",
    "education_columns = X_education.columns\n",
    "\n",
    "# bag of words features\n",
    "bow_fields = ['occupation', 'industry', 'major', 'city', 'state_or_province', 'country']\n",
    "bow_features = {}\n",
    "for field in bow_fields:\n",
    "    vectorizer = CountVectorizer()\n",
    "    bow_features[field] = vectorizer.fit_transform(df[field])\n",
    "    \n",
    "# combine covariates\n",
    "X_job_description_sparse = sp.csr_matrix(X_job_description)\n",
    "X_article_sparse = sp.csr_matrix(X_article)\n",
    "X_categorical_sparse = sp.csr_matrix(X_education)\n",
    "X_combined = sp.hstack([X_job_description_sparse, X_article_sparse, X_categorical_sparse] + [bow_features[field] for field in bow_fields])\n",
    "\n",
    "# extract targets\n",
    "y = df['is_relevant']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we fit supervised machine learning models with binary outputs over all news groups. The outputs correspond to True/False depending on whether the article in the input is relevant to the profile in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression average prediction score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# logistic regression model\n",
    "logistic_model = LogisticRegression(max_iter=1000, random_state=1)\n",
    "\n",
    "# 6-fold cross validation\n",
    "cv_logistic = cross_val_score(logistic_model, X_combined, y, scoring='f1', cv=6)\n",
    "print(f\"Logistic regression average prediction score: {cv_logistic.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes average prediction score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# bernoulli bayes model (binary classification)\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# 6-fold cross validation\n",
    "cv_nb = cross_val_score(nb, X_combined, y, scoring='f1', cv=6)\n",
    "print(f\"Naive Bayes average prediction score: {cv_nb.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree average prediction score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# 6-fold cross validation\n",
    "cv_dt = cross_val_score(dt, X_combined, y, scoring='f1', cv=6)\n",
    "print(f\"Decision tree average prediction score: {cv_dt.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM average prediction score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(random_state=1)\n",
    "\n",
    "# 6-fold cross validation\n",
    "cv_svm = cross_val_score(svm, X_combined, y, scoring='f1', cv=6)\n",
    "print(f\"SVM average prediction score: {cv_svm.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-news group classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "1) train a classifier which determines which group an article comes from\n",
    "2) train different models per news group\n",
    "3) in test set, try to guess news group of an article and then use corresponding classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we fit supervised machine learning models with binary outputs, one model per news group. The outputs correspond to True/False depending on whether the article in the input is relevant to the profile in the input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we train a classifier that tries to predict the news group of an article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression article classifier prediction score: 0.9459980713596914\n",
      "Naive Bayes article classifier prediction score: 0.9218900675024108\n",
      "Decision tree article classifier prediction score: 0.7540983606557377\n",
      "SVM article classifier prediction score: 0.944069431051109\n"
     ]
    }
   ],
   "source": [
    "# vectorize articles\n",
    "article_classifier_vectorizer = TfidfVectorizer()\n",
    "X_article_classifier = article_classifier_vectorizer.fit_transform(newsgroups_train[0])\n",
    "# obtain true labels per article\n",
    "y_article_classifier = newsgroups_train[1]\n",
    "\n",
    "# create different classifiers\n",
    "lr_article_classifier = LogisticRegression(max_iter=1000, random_state=2)\n",
    "nb_article_classifier = MultinomialNB()\n",
    "dt_article_classifier = DecisionTreeClassifier(random_state=2)\n",
    "svm_article_classifier = SVC(random_state=2)\n",
    "\n",
    "# train-test split\n",
    "X_article_classifier_train, X_article_classifier_test, \\\n",
    "    y_article_classifier_train, y_article_classifier_test \\\n",
    "        = train_test_split(X_article_classifier, y_article_classifier, \\\n",
    "        test_size=0.2, random_state=2)\n",
    "\n",
    "# fit classifiers    \n",
    "lr_article_classifier.fit(X_article_classifier_train, y_article_classifier_train) \n",
    "nb_article_classifier.fit(X_article_classifier_train, y_article_classifier_train)\n",
    "dt_article_classifier.fit(X_article_classifier_train, y_article_classifier_train)\n",
    "svm_article_classifier.fit(X_article_classifier_train, y_article_classifier_train)\n",
    "\n",
    "# obtain prediction scores\n",
    "lr_article_classifier_score = lr_article_classifier.score(X_article_classifier_test, y_article_classifier_test)\n",
    "nb_article_classifier_score = nb_article_classifier.score(X_article_classifier_test, y_article_classifier_test)\n",
    "dt_article_classifier_score = dt_article_classifier.score(X_article_classifier_test, y_article_classifier_test)\n",
    "svm_article_classifier_score = svm_article_classifier.score(X_article_classifier_test, y_article_classifier_test)\n",
    "\n",
    "print(f'Logistic regression article classifier prediction score: {lr_article_classifier_score}')\n",
    "print(f'Naive Bayes article classifier prediction score: {nb_article_classifier_score}')\n",
    "print(f'Decision tree article classifier prediction score: {dt_article_classifier_score}')\n",
    "print(f'SVM article classifier prediction score: {svm_article_classifier_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression obtains the highest prediction score, so we will use that classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_classifier = lr_article_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vectorizer for articles\n",
    "article_vectorizer = TfidfVectorizer()\n",
    "article_vectorizer.fit(df['article'])\n",
    "\n",
    "# remove pre-existing articles and labels from df\n",
    "df = df.drop(columns=['article', 'is_relevant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train different models per news group. Each model is trained on arbitrary user profiles and articles from the news group, and tries to predict whether the article is relevant to the profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate df to increase train/test size\n",
    "# any given profile can be associated with multiple different articles\n",
    "# (corresponds to different rows in the dataframe)\n",
    "df_new = pd.concat([df, df, df, df ,df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_per_newsgroup = []\n",
    "models_f1_scores_per_newsgroup = []\n",
    "\n",
    "# save vectorizers for future use in testing\n",
    "job_description_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# fit vectorizers on dataset\n",
    "job_description_vectorizer.fit(df_new['job_description_preprocessed'])\n",
    "\n",
    "bow_fields = ['occupation', 'industry', 'major', 'city', 'state_or_province', 'country']\n",
    "bow_vectorizers = {}\n",
    "for field in bow_fields:\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(df_new[field])\n",
    "    bow_vectorizers[field] = vectorizer\n",
    "\n",
    "num_profiles = len(df_new)\n",
    "\n",
    "for news_group in NEWSGROUPS:\n",
    "    \n",
    "    # fetch indices of articles with label corresponding to the newsgroup\n",
    "    newsgroup_indices = np.nonzero(newsgroups_train[1] == categories.index(news_group))[0]\n",
    "    \n",
    "    # take the articles at the given indices\n",
    "    newsgroup_articles = np.take(newsgroups_train[0], indices=newsgroup_indices)\n",
    "    \n",
    "    # sample articles randomly\n",
    "    articles = np.random.choice(newsgroup_articles, size=num_profiles)\n",
    "    \n",
    "    # add articles to dataframe\n",
    "    df_new['article'] = articles    \n",
    "    \n",
    "    # add is_relevant field\n",
    "    # True if profile is from the same news group\n",
    "    df_new['is_relevant'] = df_new['news_group'] == news_group \n",
    "    \n",
    "    # process data\n",
    "    # vectorization for job description\n",
    "    X_job_description = job_description_vectorizer.transform(df_new['job_description_preprocessed'])\n",
    "    X_article = article_vectorizer.transform(df_new['article'])\n",
    "    \n",
    "    # one-hot encoding for education\n",
    "    X_education = pd.get_dummies(df_new['education'])\n",
    "\n",
    "    # bag of words features\n",
    "    bow_features = {}\n",
    "    for field in bow_fields:\n",
    "        bow_features[field] = bow_vectorizers[field].transform(df_new[field])\n",
    "        \n",
    "    # combine covariates\n",
    "    X_job_description_sparse = sp.csr_matrix(X_job_description)\n",
    "    X_article_sparse = sp.csr_matrix(X_article)\n",
    "    X_categorical_sparse = sp.csr_matrix(X_education)\n",
    "    X_combined = sp.hstack([X_job_description_sparse, X_article_sparse, X_categorical_sparse] + [bow_features[field] for field in bow_fields])\n",
    "\n",
    "    # extract targets\n",
    "    y = df_new['is_relevant']\n",
    "    \n",
    "    # train-test split\n",
    "    indices = np.arange(num_profiles)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=2, stratify=y)\n",
    "    X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X_combined, y, indices, test_size=0.2, random_state=2, stratify=y)\n",
    "    \n",
    "    # create model\n",
    "    lr = LogisticRegression(max_iter=1000, random_state=2)\n",
    "\n",
    "    # fit model\n",
    "    lr.fit(X_train, y_train) \n",
    "    \n",
    "    # obtain predictions\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "    \n",
    "    # obtain f1 scores\n",
    "    lr_f1 = f1_score(y_test, y_pred_lr)\n",
    "    \n",
    "    # add model to list\n",
    "    models_per_newsgroup.append(lr)\n",
    "    models_f1_scores_per_newsgroup.append(lr_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the model F1 scores per news group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: 0.9189189189189189\n",
      "comp.windows.x: 0.9743589743589743\n",
      "misc.forsale: 1.0\n",
      "rec.autos: 0.9743589743589743\n",
      "sci.med: 1.0\n",
      "rec.sport.hockey: 1.0\n",
      "sci.space: 1.0\n",
      "soc.religion.christian: 1.0\n",
      "talk.politics.guns: 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(NEWSGROUPS)):\n",
    "    print(f'{NEWSGROUPS[i]}: {models_f1_scores_per_newsgroup[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now make use of the pipeline to predict relevancy of profiles and articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve test instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose random articles from newsgroups_test, one for each profile\n",
    "test_indices = np.random.choice(num_articles_newsgroups_test, size=len(df))\n",
    "\n",
    "# attach articles and their labels to df\n",
    "test_articles = np.array(newsgroups_test[0])[test_indices]\n",
    "test_labels = np.array(newsgroups_test[1])[test_indices]\n",
    "\n",
    "# store article and article's true news group\n",
    "df['article'] = test_articles\n",
    "df['true_article_news_group'] = np.array(categories)[test_labels]\n",
    "\n",
    "# predict article news group\n",
    "X_test_articles = article_classifier_vectorizer.transform(df['article'])\n",
    "predicted_news_groups = article_classifier.predict(X_test_articles)\n",
    "df['predicted_article_news_group'] = np.array(categories)[predicted_news_groups]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group data by predicted article news groups and test on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('predicted_article_news_group')\n",
    "\n",
    "true_ys = []\n",
    "predicted_ys = []\n",
    "\n",
    "for news_group, group in grouped:\n",
    "    \n",
    "    news_group_index = categories.index(news_group)\n",
    "    model = models_per_newsgroup[news_group_index]\n",
    "    \n",
    "    # process data\n",
    "    # vectorization for job description\n",
    "    X_job_description = job_description_vectorizer.transform(group['job_description_preprocessed'])\n",
    "    X_article = article_vectorizer.transform(group['article'])\n",
    "    \n",
    "    # one-hot encoding for education\n",
    "    X_education = pd.get_dummies(group['education']).reindex(columns=education_columns, fill_value=False)\n",
    "\n",
    "    # bag of words features\n",
    "    bow_features = {}\n",
    "    for field in bow_fields:\n",
    "        bow_features[field] = bow_vectorizers[field].transform(group[field])\n",
    "        \n",
    "    # combine covariates\n",
    "    X_job_description_sparse = sp.csr_matrix(X_job_description)\n",
    "    X_article_sparse = sp.csr_matrix(X_article)\n",
    "    X_categorical_sparse = sp.csr_matrix(X_education)\n",
    "    X_combined = sp.hstack([X_job_description_sparse, X_article_sparse, X_categorical_sparse] + [bow_features[field] for field in bow_fields])\n",
    "\n",
    "    # extract targets\n",
    "    # True if profile's news group is equal to article news group\n",
    "    y = group['news_group'] == group['true_article_news_group']\n",
    "    \n",
    "    # predict\n",
    "    y_pred = model.predict(X_combined)\n",
    "    \n",
    "    # add values to lists\n",
    "    true_ys.append(y)\n",
    "    predicted_ys.append(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get prediction accuracy by comparing the lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy: 0.988950276243094\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "true_y_array = np.concatenate(true_ys)\n",
    "predicted_y_array = np.concatenate(predicted_ys)\n",
    "    \n",
    "acc = accuracy_score(true_y_array, predicted_y_array)\n",
    "print(f'Prediction accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change if don't want to save metrics to tables/supervised\n",
    "SAVE_METRICS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model metrics\n",
    "df_pairwise = pd.DataFrame(index=['Logistic', 'Naive Bayes', 'Decision tree', 'SVM'], \n",
    "                             columns=['Average prediction score'])\n",
    "df_pairwise.loc['Logistic', :] = cv_logistic.mean()\n",
    "df_pairwise.loc['Naive Bayes', :] = cv_nb.mean()\n",
    "df_pairwise.loc['Decision tree', :] = cv_dt.mean()\n",
    "df_pairwise.loc['SVM', :] = cv_svm.mean()\n",
    "\n",
    "if SAVE_METRICS:\n",
    "    df_pairwise.to_csv('../tables/supervised/supervised_scores.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
